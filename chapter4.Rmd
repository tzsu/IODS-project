# Clustering and Classification (week 4)

*This week we will look at the dataset, "Boston", which already exists inside R, and perform clustering and classification on the data. Also, data wrangling for next week deals with two datasets, "Human development" and "Gender inequality".*

## 1. Data wrangling for next week
*you can see it in my github:*
<https://github.com/tzsu/IODS-project/blob/master/data/create_human.R>

## 2. Analysis 

### 1. about the data: Housing Values in Suburbs of Boston

#### It is from two sources
- 1. Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81¡V102.
- 2. Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.

#### The Boston data frame has 506 observations and 14 variables. The following are the variables:

- **crim** - per capita crime rate by town
- **zn** - proportion of residential land zoned for lots over 25,000 sq.ft
- **indus** - proportion of non-retail business acres per town
- **chas** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- **nox** - nitrogen oxides concentration (parts per 10 million)
- **rm** - average number of rooms per dwelling
- **age** - proportion of owner-occupied units built prior to 1940
- **dis** - weighted mean of distances to five Boston employment centres
- **rad** - index of accessibility to radial highways
- **tax** - full-value property-tax rate per $10,000
- **ptratio** - pupil-teacher ratio by town
- **black** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- **lstat** - lower status of the population (%)
- **medv** - median value of owner-occupied homes in \$1000s



```{r, include=FALSE}
# access the packages
library(MASS)
library(dplyr)
library(ggplot2)
library(GGally)
```

```{r, echo=TRUE}
# load the data
data("Boston")
# explore the dataset
str(Boston)
summary(Boston)
```

### 2. Visualising the data
#### Pairing up all the variables cannot really show anything to us. I tried accessing library(corrplot), but the package cannot be downloaded by my r version :(

```{r, echo=TRUE}
# plot matrix of the variables
pairs(Boston)
```

#### From looking at the correlations plot in datacamp, you can see which variables are highly correlated and which are not.

```{r, echo=TRUE}
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston)%>%round(2)
# print the correlation matrix
cor_matrix
```

### 3. Standardizing the dataset

#### Standardizing the data we subtract the column means from the corresponding columns and divide the difference with standard deviation. The means of the variables will then all become zero.
```{r, echo=TRUE}
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

#### Now create a categorical variable of the crime rate, and use the quantiles as the break points in the categorical variables. Then drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.

```{r, echo=FALSE}
# following what we did in datacamp
# save the scaled crim as scaled_crim
scaled_crim <- c(boston_scaled$crim)

# see summary
summary(scaled_crim)
# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)
bins
# create a categorical variable 'crime'
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label = c("low","med_low","med_high","high"))
# look at the table of the new factor crime
table(boston_scaled$crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# boston_scaled is available
nrow(boston_scaled)
# number of rows in the Boston dataset 
n <- 506
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

### 4. Fitting the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. 

```{r, echo=TRUE}
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
```

#### Look at the LDA (bi)plot.
```{r, echo=TRUE}
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 3)
```

### 5. Predicting 
#### The results show that the classifier predict the crime rates quite correctly.
```{r, echo=TRUE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
