learning2014 <- filter(learning2014, Points > 0)
#read the structure of the new dataset, and see that there are 166 obs. and 7 variables
str(learning2014)
#change column names, make them all lowercase
colnames(learning2014)
colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"
#Set the working directory
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
#save the new dataset in the 'data' folder as 'learning2014.txt'
write.table(learning2014, "learning2014.txt", row.names = FALSE)
#read 'learning2014' into R
#show the structure and first 6 obs. of the data
read.table("learning2014.txt", header = TRUE)
str(learning2014)
head(learning2014)
View(learning2014)
View(learning2014)
View(lrn14)
View(lrn14)
pairs(learning2014[-1], col = learning2014$gender)
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
install.packages("GGally")
install.packages("ggplot2")
pairs(learning2014[-1], col = learning2014$gender)
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
p
my_model2 <- lm(points ~ attitude + stra + surf, data = learning2014)
# print out a summary of the model
summary(my_model2)
my_model2 <- lm(points ~ attitude + stra + deep, data = learning2014)
# print out a summary of the model
#
summary(my_model2)
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
#print out the summary of the new model
summary(my_model2)
# create a regression model with multiple explanatory variables
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
?plot.lm
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
#Tzu-lin Su, Jan 30
#Week 2 Rstudio, Regression and model validation
#Read the data into memory
#There are 183 rows and 60 columns in the learning 2014 table.
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)
#access the dplyr library
library(dplyr)
#questions related to deep/surface/strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
#select the columns related to deep/surface/strategic and create columns 'deep'/'surf'/stra'
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
#Rescale attitude
lrn14$Attitude / 10
lrn14$attitude <- lrn14$Attitude / 10
#select the columns that we want to analyse
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
learning2014 <- select(lrn14, one_of(keep_columns))
#change column names, make them all lowercase
colnames(learning2014)
colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"
#exclude obs where point is 0
learning2014 <- filter(learning2014, points > 0)
#Set the working directory
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
#save the new dataset in the 'data' folder as 'learning2014.txt'
write.table(learning2014, "learning2014.txt", row.names = FALSE)
#read 'learning2014' into R
#show the structure and first 6 obs. of the data
read.table("learning2014.txt", header = TRUE)
str(learning2014)
head(learning2014)
#The learning2014 dataset is prepared to help us analyse the relationship between learning approaches and students achievements in an introductory statistics course in Finland.
#The dataset includes 7 variables (gender, age, attitude, deep, strategic, and surface), and 166 observations where students have positive points.
# a scatter plot matrix of the variables in learning2014.
# [-1] excludes the first column (gender)
pairs(learning2014[-1], col = learning2014$gender)
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
# The graphical overview show summaries of the variables in the data.
# There are aprroximately double the amount of female students than male students. Most of the students are under 30 years old. And attitude seems to correlate with points.
p
# create a regression model with three explanatory variables, attitude, stra and deep. Let points be the target variable.
my_model2 <- lm(points ~ attitude + stra + deep, data = learning2014)
# print out a summary of the model
summary(my_model2)
# From the summary of the output, it shows that the multiple regression equation to be around 'points = 11.39 + 3.53(attitude) + 0.96(stra) - 0.75(deep)'.
# Which means if the value attitude increase by one, points will increase by 3.53 (if other variables stay the same), and so on. When all three explanatory variables are 0, points is predicted to be 11.39.(This may be because we have exclude points=0)
# From F test for overall significance, we can conclude that at least one explanatory variable affects 'points'.
# The t test shows that there is evidence that 'attitude' affects 'points', however, it is also quite clear that 'deep' has no linear relationship with 'points'.
# Remove 'deep' and create a new regression model.
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
# print out the summary of the new model
summary(my_model2)
# The new summary shows that the regression equation 'points = 8.97 + 3.47(attitude) + 0.91(stra)'. Both the explanatory variables have positive affects on the target variable.
# Multiple R-squared here indicates that 20.5% of the variation in points is explained by the variation in attitude and stra.
# create a regression model with multiple explanatory variables
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
?plot.lm
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
# The linear regression model assumes that:
# 1. Errors are normally distributed.
# 2. Errors are not correlated.
# 3. Errors have constant variance.
# 4. The size of a given error does not depend on the explanatory variables
# The Residuals vs Fitted values plot here has no pattern, which supports the assumption of constant variance of errors and the size of the errors do not depend on the explanatory variables.
# The Normal QQ plot here shows that a resonable fit with the line, which means the errors are normally distributed.
# The Residuals vs Leverage plot can measure how much impact a single observation has on the model. Here it is quite scattered, but all are withen 0.06, which means no outliers. (We have already eliminate the observations where points = 0)
read.table("learning2014.txt", header = TRUE)
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project")
render("chapter2.Rmd", output_file = "report.html")
rmarkdown::render("chapter2.Rmd", output_file = "report.html")
rmarkdown::render("chapter2.Rmd", output_file = "chapter2.html")
#read 'learning2014' into R
#show the structure and first 6 obs. of the data
read.table("learning2014.txt", header = TRUE)
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project")
#Tzu-lin Su, Jan 30
#Week 2 Rstudio, Regression and model validation
#Read the data into memory
#There are 183 rows and 60 columns in the learning 2014 table.
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
dim(lrn14)
str(lrn14)
#access the dplyr library
library(dplyr)
#questions related to deep/surface/strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
#select the columns related to deep/surface/strategic and create columns 'deep'/'surf'/stra'
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
#Rescale attitude
lrn14$Attitude / 10
lrn14$attitude <- lrn14$Attitude / 10
#select the columns that we want to analyse
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
learning2014 <- select(lrn14, one_of(keep_columns))
#change column names, make them all lowercase
colnames(learning2014)
colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"
#exclude obs where point is 0
learning2014 <- filter(learning2014, points > 0)
#Set the working directory
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project")
#save the new dataset in the 'data' folder as 'learning2014.txt'
write.table(learning2014, "learning2014.txt", row.names = FALSE)
#read 'learning2014' into R
#show the structure and first 6 obs. of the data
read.table("learning2014.txt", header = TRUE)
str(learning2014)
head(learning2014)
#The learning2014 dataset is prepared to help us analyse the relationship between learning approaches and students achievements in an introductory statistics course in Finland.
#The dataset includes 7 variables (gender, age, attitude, deep, strategic, and surface), and 166 observations where students have positive points.
# a scatter plot matrix of the variables in learning2014.
# [-1] excludes the first column (gender)
pairs(learning2014[-1], col = learning2014$gender)
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)
# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
# draw the plot
# The graphical overview show summaries of the variables in the data.
# There are aprroximately double the amount of female students than male students. Most of the students are under 30 years old. And attitude seems to correlate with points.
p
# create a regression model with three explanatory variables, attitude, stra and deep. Let points be the target variable.
my_model2 <- lm(points ~ attitude + stra + deep, data = learning2014)
# print out a summary of the model
summary(my_model2)
# From the summary of the output, it shows that the multiple regression equation to be around 'points = 11.39 + 3.53(attitude) + 0.96(stra) - 0.75(deep)'.
# Which means if the value attitude increase by one, points will increase by 3.53 (if other variables stay the same), and so on. When all three explanatory variables are 0, points is predicted to be 11.39.(This may be because we have exclude points=0)
# From F test for overall significance, we can conclude that at least one explanatory variable affects 'points'.
# The t test shows that there is evidence that 'attitude' affects 'points', however, it is also quite clear that 'deep' has no linear relationship with 'points'.
# Remove 'deep' and create a new regression model.
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
# print out the summary of the new model
summary(my_model2)
# The new summary shows that the regression equation 'points = 8.97 + 3.47(attitude) + 0.91(stra)'. Both the explanatory variables have positive affects on the target variable.
# Multiple R-squared here indicates that 20.5% of the variation in points is explained by the variation in attitude and stra.
# create a regression model with multiple explanatory variables
my_model2 <- lm(points ~ attitude + stra, data = learning2014)
?plot.lm
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
# The linear regression model assumes that:
# 1. Errors are normally distributed.
# 2. Errors are not correlated.
# 3. Errors have constant variance.
# 4. The size of a given error does not depend on the explanatory variables
# The Residuals vs Fitted values plot here has no pattern, which supports the assumption of constant variance of errors and the size of the errors do not depend on the explanatory variables.
# The Normal QQ plot here shows that a resonable fit with the line, which means the errors are normally distributed.
# The Residuals vs Leverage plot can measure how much impact a single observation has on the model. Here it is quite scattered, but all are withen 0.06, which means no outliers. (We have already eliminate the observations where points = 0)
plot(my_model2, which = c(1,2,5), cex.caption = 1)
plot(my_model2, which = c(1,2,5), cex.caption = 2)
(my_model2, which = c(1,2,5), cex.caption = 3)
#Tzu-lin Su
#Date: February 8, 2017
#Week 3: Logistic regression
#Data source: UCI Machine Learning Reposiory - Student Alcohol Consumption
#P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.
#set working directory
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
#Read student-mat and student-por into R
#Explore the structure and dimensions of the data
math <- read.table("student-mat.csv", sep = ';', header = TRUE)
por <- read.table("student-por.csv", sep = ';', header = TRUE)
glimpse(math)
glimpse(por)
#access the dplyr library
library(dplyr)
#Join the two data sets using the common columns to use as identifiers
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
#join the two datasets by the selected identifiers
#see structure and dimention of the new table math-por
math_por <- inner_join(math, por, by = join_by, suffix = c(".math",".por"))
glimpse(math_por)
#copy from the if-else datacamp exercise
# dplyr, math_por, join_by are available
# print out the column names of 'math_por'
colnames(math_por)
# create a new data frame with only the joined columns
alc <- select(math_por, one_of(join_by))
# the columns in the datasets which were not used for joining the data
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
# print out the columns not used for joining
print(notjoined_columns)
# for every column name not used for joining...
for(column_name in notjoined_columns) {
# select two columns from 'math_por' with the same original name
two_columns <- select(math_por, starts_with(column_name))
# select the first column vector of those two columns
first_column <- select(two_columns, 1)[[1]]
# if that first column vector is numeric...
if(is.numeric(first_column)) {
# take a rounded average of each row of the two columns and
# add the resulting vector to the alc data frame
alc[column_name] <- round(rowMeans(two_columns))
} else { # else if it's not numeric...
# add the first column vector to the alc data frame
alc[column_name] <- first_column
}
}
# glimpse at the new combined data
glimpse(alc)
# access the 'tidyverse' packages dplyr and ggplot2
library(dplyr)
library(ggplot2)
# define a new column alc_use by combining weekday and weekend alcohol use
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
# define a new logical column 'high_use'
alc <- mutate(alc, high_use = alc_use > 2)
#glimpse at the joined and modified data. the joined data should now have 382 obs and 35 variables.
glimpse(alc)
#save the new dataset in the 'data' folder as 'alc.txt'
write.table(alc, "alc.txt", row.names = FALSE)
#end of data wrangling part
#Read the data into memory
#There are 382 obs. and 35 variables in the acl table.
View(math)
View(math_por)
alc <- read.table('data/alc.csv', sep ="\t", header = TRUE)
alc <- read.table("alc.csv", sep ="\t", header = TRUE)
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
alc <- read.table("alc.csv", sep ="\t", header = TRUE)
alc <- as.data.frame(read.table('data/alc.csv', sep ="\t", header = TRUE))
alc <- as.data.frame(read.table('data/alc.txt', sep ="\t", header = TRUE))
write.table(alc, file = "alc.csv", sep = "\t", col.names = TRUE)
alc <- read.table('data/alc.csv', sep ="\t", header = TRUE)
alc <- read.table('data/alc.csv', sep ="\t", header = TRUE)
alc <- as.data.frame(read.table('data/alc.csv', sep ="\t", header = TRUE))
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
alc <- as.data.frame(read.table('data/alc.csv', sep ="\t", header = TRUE))
write.table(alc, file = "alc.csv", sep = "\t", col.names = TRUE)
alc <- read.table("alc.csv", sep ="\t", header = TRUE)
str(alc)
dim(alc)
glimpse(alc)
#Tzu-lin Su
#Date: February 8, 2017
#Week 3: Logistic regression
#Data source: UCI Machine Learning Reposiory - Student Alcohol Consumption
#P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.
#set working directory
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project/data")
#access the dplyr library
library(dplyr)
#Read student-mat and student-por into R
#Explore the structure and dimensions of the data
math <- read.table("student-mat.csv", sep = ';', header = TRUE)
por <- read.table("student-por.csv", sep = ';', header = TRUE)
glimpse(math)
glimpse(por)
#Join the two data sets using the common columns to use as identifiers
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
#join the two datasets by the selected identifiers
#see structure and dimention of the new table math-por
math_por <- inner_join(math, por, by = join_by, suffix = c(".math",".por"))
glimpse(math_por)
#copy from the if-else datacamp exercise
# dplyr, math_por, join_by are available
# print out the column names of 'math_por'
colnames(math_por)
# create a new data frame with only the joined columns
alc <- select(math_por, one_of(join_by))
# the columns in the datasets which were not used for joining the data
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
# print out the columns not used for joining
print(notjoined_columns)
# for every column name not used for joining...
for(column_name in notjoined_columns) {
# select two columns from 'math_por' with the same original name
two_columns <- select(math_por, starts_with(column_name))
# select the first column vector of those two columns
first_column <- select(two_columns, 1)[[1]]
# if that first column vector is numeric...
if(is.numeric(first_column)) {
# take a rounded average of each row of the two columns and
# add the resulting vector to the alc data frame
alc[column_name] <- round(rowMeans(two_columns))
} else { # else if it's not numeric...
# add the first column vector to the alc data frame
alc[column_name] <- first_column
}
}
# glimpse at the new combined data
glimpse(alc)
# access the 'tidyverse' packages dplyr and ggplot2
library(dplyr)
library(ggplot2)
# define a new column alc_use by combining weekday and weekend alcohol use
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
# define a new logical column 'high_use'
alc <- mutate(alc, high_use = alc_use > 2)
#glimpse at the joined and modified data. the joined data should now have 382 obs and 35 variables.
glimpse(alc)
#save the new dataset in the 'data' folder as 'alc.txt'
write.table(alc, file = "alc.csv", sep = "\t", col.names = TRUE)
#end of data wrangling part
alc <- read.table("alc.csv", sep ="\t", header = TRUE)
#Taking a glimpse at the combined dataset
glimpse(alc)
setwd("D:/Sydney Uni/forth semester units of study/Intro to Open Data Science/IODS-project")
#Taking a glimpse at the combined dataset
glimpse(alc)
library(ggplot2)
# initialize a plot of high_use and G3
g1 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))
# define the plot as a boxplot and draw it
g1 + geom_boxplot() + ylab("grade")
# initialise a plot of high_use and absences
g2 <- ggplot(alc, aes(x= high_use, y = absences, col = sex))
# define the plot as a boxplot and draw it
g2 +geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
library(ggplot2)
# initialize a plot of high_use and age
g1 <- ggplot(alc, aes(x = high_use, y = age, col = sex))
# define the plot as a boxplot and draw it
g1 + geom_boxplot() + ylab("age") + ggtitle("Student age by alcohol consumption and sex")
# initialise a plot of high_use and final grade
g2 <- ggplot(alc, aes(x= high_use, y = G3, col = sex))
# define the plot as a boxplot and draw it
g2 +geom_boxplot() + ggtitle("Student final grade by alcohol consumption and sex")
g4 <- ggplot(alc, aes(age)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('age')
#define the plot as a barplot and draw it
g4 + geom_barplot() + ggtitle("Student age by alcohol consumption and sex")
g4 <- ggplot(alc, aes(age)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('age')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student age by alcohol consumption and sex")
g4 <- ggplot(alc, aes(age)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('age')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student age by alcohol consumption")
g4 <- ggplot(alc, aes(freetime)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('freetime')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
multiplot(g1, g2, g4, cols = 2)
g4 <- ggplot(alc, aes(freetime)) + geom_bar(aes(fill = high_use), col = high_use, position = "dodge", stat="count") + xlab('freetime')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
g4 <- ggplot(alc, aes(freetime), col=high_use) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('freetime')
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
g1 <- ggplot(alc, aes(x = high_use, y = G3, col=sex)) + geom_boxplot() + ylab('final grade') + xlab('high_use')
# define the plot as a boxplot and draw it
g1 + geom_boxplot() + ylab("grade")+ ggtitle("Student final grades by alcohol consumption and sex")
# initialise a plot of high_use and absences
g2 <- ggplot(alc, aes(x= high_use, y = absences, col = sex))
# define the plot as a boxplot and draw it
g2 +geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
#initialise a plot of high_use and freetime
g4 <- ggplot(alc, aes(freetime, col=high_use)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('freetime')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
#initialise a plot of high_use and freetime
g4 <- ggplot(alc, aes(freetime, fill=high_use)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('freetime')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
g1 <- ggplot(alc, aes(x = high_use, y = G3, col=sex)) + geom_boxplot() + ylab('final grade') + xlab('high_use')
# define the plot as a boxplot and draw it
g1 + geom_boxplot() + ylab("grade")+ ggtitle("Student final grades by alcohol consumption and sex")
# initialise a plot of high_use and absences
g2 <- ggplot(alc, aes(x= high_use, y = absences, col = sex))
# define the plot as a boxplot and draw it
g2 +geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
#initialise a plot of high_use and freetime
g4 <- ggplot(alc, aes(freetime, fill=high_use)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('freetime')
#define the plot as a barplot and draw it
g4 + geom_bar() + ggtitle("Student freetime by alcohol consumption")
g4 <- ggplot(alc, aes(freetime)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count")
#define the plot as a barplot and draw it
g4  + ggtitle("Student freetime by alcohol consumption")
g1 <- ggplot(alc, aes(x = high_use, y = G3, col=sex)) + geom_boxplot() + ylab('final grade') + xlab('high_use')
g1 + geom_boxplot() + ylab("grade")+ ggtitle("Student final grades by alcohol consumption and sex")
g2 <- ggplot(alc, aes(x= high_use, y = absences, col = sex))
g2 +geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
g4 <- ggplot(alc, aes(romantic)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count")
g4  + ggtitle("Student romantic by alcohol consumption")
g5 <- ggplot(alc, aes(romantic)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count")
#define the plot as a barplot and draw it
g5  + ggtitle("Student romantic by alcohol consumption")
g4 <- ggplot(alc, aes(freetime)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count")
#define the plot as a barplot and draw it
g4  + ggtitle("Student freetime by alcohol consumption")
# find the model with glm()
m <- glm(high_use ~ G3 + absences + sex + freetime , data = alc, family = "binomial")
# print out a summary of the model
summary(m)
# print out the coefficients of the model
coef(m)
OR <- coef(m) %>%exp
#compute confidence intervals(CI)
CI <- exp(confint(m))
# predict() the probability of high_use
probabilities <- predict(m, type = "response")
# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability>0.5)
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)
m <- glm(high_use ~ G3 + absences + freetime + romantic, data = alc, family = "binomial")
# print out a summary of the model
summary(m)
# find the model with glm()
m <- glm(high_use ~ G3 + absences + freetime + romantic, data = alc, family = "binomial")
# print out a summary of the model
summary(m)
# compute odds ratios (OR)
OR <- coef(m) %>%exp
#compute confidence intervals(CI)
CI <- exp(confint(m))
# find the new model with glm()
m <- glm(high_use ~ absences + freetime, data = alc, family = "binomial")
# print out a summary of the model
summary(m)
# predict() the probability of high_use
probabilities <- predict(m, type = "response")
# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)
# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability>0.5)
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)
# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))
# define the geom as points and draw the plot
g + geom_point()
# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)%>% prop.table()%>% addmargins
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$probability)
